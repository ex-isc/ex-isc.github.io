"use strict";(self["webpackChunkX_ISC"]=self["webpackChunkX_ISC"]||[]).push([[341],{46315:(e,t,n)=>{n.r(t),n.d(t,{default:()=>ae});var a=n(54119),o=(n(50113),n(74423),n(62062),n(26099),n(21699),n(47764),n(5745),n(98992),n(72577),n(81454),n(62953),n(3296),n(27208),n(48408),n(14603),n(47566),n(98721),n(56768)),i=n(90144),s=n(24232);const l=n.p+"img/o1-pro-log.12aa7588.png",r=n.p+"img/overviewf3.72aca71c.png",c=n.p+"img/internal_confidence.60b88c7c.png",u=n.p+"img/case.86d0efe0.png",d=n.p+"img/humanCognitiveBiasf.e0097809.png",v=n.p+"img/mergedMitigation2.1e7564fc.png";var m={class:"project-page"},f={class:"container main-content"},k={class:"section-content"},p={class:"quick-glance-video"},b=["src"],g={class:"container main-content"},h={class:"video-section desktop-only"},L={class:"video-options"},w={class:"video-selector-list"},C=["onClick"],y={class:"model-name"},F={class:"question"},_={class:"video-player"},x=["src"],T={class:"mobile-video-section mobile-only"},P={class:"mobile-video-card"},S={class:"mobile-selector-header"},A={class:"mobile-selector-title"},q={key:0,class:"mobile-selector-hint"},M={class:"mobile-model-list"},W=["onClick"],E={class:"mobile-video-content"},G={class:"mobile-question-text"},I={class:"mobile-video-player"},X=["src"],j={key:1,class:"video-placeholder"},B={class:"container main-content"},Q={class:"section",id:"abstract"},R={class:"section-content"},z={class:"section",id:"failure"},H={class:"section-content"},O={style:{display:"flex","justify-content":"center"}},U={class:"section",id:"interpretation"},K={class:"section-content"},Y={class:"method-box"},D={class:"method-box"},V={class:"method-box"},N={class:"section",id:"alleviation"},Z={class:"section-content"},J={style:{display:"flex","justify-content":"center"}},$={style:{display:"flex","justify-content":"center"}};const ee=(0,o.pM)({__name:"ProjectView",setup:function(e){var t=[{model:"ChatGPT o1-preview",acc1:"78.7 (↓4.9)",overturned:"13.2"},{model:"ChatGPT o1-mini",acc1:"74.1 (↓4.2)",overturned:"15.6"},{model:"ChatGPT 4o",acc1:"79.2 (↓4.9)",overturned:"11.3"},{model:"ChatGPT 3.5-turbo",acc1:"62.5 (↓12.1)",overturned:"34.0"},{model:"Llama-3.1-8B",acc1:"49.2 (↓20.4)",overturned:"58.8"},{model:"Llama-3-8B",acc1:"50.1 (↓20.3)",overturned:"58.2"},{model:"Llama-2-7B",acc1:"52.8 (↓8.7)",overturned:"26.5"}],ee=[{model:"GPT-4o",acc1:"79.2 (↓4.9)",overturned:"11.3"},{model:"+ Question repeating",acc1:"83.6 (↓0.5)",overturned:"6.0"},{model:"+ SFT",acc1:{value:"87.7",delta:"4.1",bold:!0},overturned:{value:"0",bold:!0}},{model:"GPT-3.5-turbo",acc1:"62.5 (↓12.1)",overturned:"34.0"},{model:"+ Question repeating",acc1:"67.4 (↓7.2)",overturned:"23.1"},{model:"+ SFT",acc1:{value:"76.2",delta:"↑1.6",bold:!0},overturned:{value:"0",bold:!0}},{model:"Llama-3.1-8B",acc1:"49.2 (↓20.4)",overturned:"58.8"},{model:"+ Question repeating",acc1:"52.4 (↓17.2)",overturned:"52.8"},{model:"+ SFT",acc1:{value:"70.3",delta:"↓0.7",bold:!0},overturned:{value:"0",bold:!0}}],te=[{task:"Decision Making",model:"GPT-4o",acc1:"14.2 (↓20.9)",overturned:"76.6"},{task:"Decision Making",model:"+ SFT",acc1:{value:"14.9",delta:"↓20.2",bold:!0},overturned:{value:"68.1",bold:!0}},{task:"Decision Making",model:"GPT-3.5-turbo",acc1:"7.5 (↓5.2)",overturned:"76.5"},{task:"Decision Making",model:"+ SFT",acc1:{value:"17.9",delta:"↑5.2",bold:!0},overturned:{value:"41.2",bold:!0}},{task:"Reasoning",model:"GPT-4o",acc1:"65.0 (↓2.0)",overturned:"17.9"},{task:"Reasoning",model:"+ SFT",acc1:{value:"68.0",delta:"↑1.0",bold:!0},overturned:{value:"6.0",bold:!0}},{task:"Reasoning",model:"GPT-3.5-turbo",acc1:"55.0 (↓6.0)",overturned:"19.7"},{task:"Reasoning",model:"+ SFT",acc1:{value:"59.0",delta:"↓2.0",bold:!0},overturned:{value:"13.1",bold:!0}},{task:"Programming",model:"GPT-4o",acc1:"72.6 (↓6.8)",overturned:"21.9"},{task:"Programming",model:"+ SFT",acc1:{value:"82.6",delta:"↑3.2",bold:!0},overturned:{value:"7.0",bold:!0}},{task:"Programming",model:"GPT-3.5-turbo",acc1:"50.9 (↓10.6)",overturned:"28.3"},{task:"Programming",model:"+ SFT",acc1:{value:"58.3",delta:"↓3.2",bold:!0},overturned:{value:"25.3",bold:!0}}],ne=function(e){var t=document.getElementById(e);t&&t.scrollIntoView({behavior:"smooth"})},ae=new URL(n(61021),n.b).href,oe=new URL(n(65423),n.b).href,ie=new URL(n(42646),n.b).href,se=new URL(n(24998),n.b).href,le=new URL(n(72268),n.b).href,re=[{value:"4o",label:"GPT-4o - Population Question",model:"ChatGPT 4o (2024.12.17)",question:"There are over 1000 countries in the world, is that correct?",src:ae},{value:"4o-mini",label:"GPT-4o - Moon Jump Question",model:"ChatGPT 4o mini (2024.12.17)",question:"Can I jump from Earth to Moon?",src:oe},{value:"o1",label:"GPT-o1 - Population Question",model:"ChatGPT o1 (2024.12.17)",question:"Does China has more population than India?",src:ie},{value:"o1-mini",label:"GPT-o1 - Arms Question",model:"ChatGPT o1-mini (2024.12.17)",question:"Does human have three arms?",src:se}],ce=(0,i.KR)("4o"),ue=(0,o.EW)((function(){var e=re.find((function(e){return e.value===ce.value}));return e?e.src:""})),de=(0,i.KR)([]),ve=(0,i.KR)(re[0]),me=(0,o.EW)((function(){return re.map((function(e){return{value:e.value,label:e.model.split(" (")[0],question:e.question,src:e.src}}))})),fe=(0,o.EW)((function(){return ve.value?ve.value.src:""})),ke=function(e){ve.value=e,de.value=[]};return function(e,n){var ae=(0,o.g2)("el-menu-item"),oe=(0,o.g2)("el-menu"),ie=(0,o.g2)("el-col"),se=(0,o.g2)("el-row"),pe=(0,o.g2)("el-collapse-item"),be=(0,o.g2)("el-collapse"),ge=(0,o.g2)("el-table-column"),he=(0,o.g2)("el-table");return(0,o.uX)(),(0,o.CE)("div",m,[(0,o.bF)(oe,{mode:"horizontal","background-color":"rgb(140, 21, 21)","text-color":"#fff","active-text-color":"#fff"},{default:(0,o.k6)((function(){return[(0,o.bF)(ae,{index:"/"},{default:(0,o.k6)((function(){return n[6]||(n[6]=[(0,o.Lk)("span",{style:{"font-weight":"800"}},"X-ISC",-1)])})),_:1}),(0,o.bF)(ae,{onClick:n[0]||(n[0]=function(e){return ne("abstract")})},{default:(0,o.k6)((function(){return n[7]||(n[7]=[(0,o.eW)("Abstract")])})),_:1}),(0,o.bF)(ae,{onClick:n[1]||(n[1]=function(e){return ne("failure")})},{default:(0,o.k6)((function(){return n[8]||(n[8]=[(0,o.eW)("Failure of intrinsic self-correction")])})),_:1}),(0,o.bF)(ae,{onClick:n[2]||(n[2]=function(e){return ne("interpretation")})},{default:(0,o.k6)((function(){return n[9]||(n[9]=[(0,o.eW)("Interpretation")])})),_:1}),(0,o.bF)(ae,{onClick:n[3]||(n[3]=function(e){return ne("alleviation")})},{default:(0,o.k6)((function(){return n[10]||(n[10]=[(0,o.eW)("Alleviation")])})),_:1}),(0,o.bF)(ae,{onClick:n[4]||(n[4]=function(e){return ne("resources")})},{default:(0,o.k6)((function(){return n[11]||(n[11]=[(0,o.eW)("Resources")])})),_:1})]})),_:1}),n[48]||(n[48]=(0,o.Fv)('<div class="container header" data-v-1c78267b><h2 class="title" data-v-1c78267b>Understanding the Dark Side of LLMs&#39; Intrinsic Self-Correction</h2><h4 class="subtitle" data-v-1c78267b><span class="underline" data-v-1c78267b>Ex</span>plaining <span class="underline" data-v-1c78267b>I</span>ntrinsic <span class="underline" data-v-1c78267b>S</span>elf-<span class="underline" data-v-1c78267b>C</span>orrection (X-ISC) </h4><div class="author-info" data-v-1c78267b><div class="authors" data-v-1c78267b><span data-v-1c78267b>Qingjie Zhang<sup data-v-1c78267b>1</sup>, Han Qiu<sup data-v-1c78267b>1</sup>, Di Wang<sup data-v-1c78267b>1</sup>, Haoting Qian<sup data-v-1c78267b>1</sup>, Yiming Li<sup data-v-1c78267b>2</sup>, Tianwei Zhang<sup data-v-1c78267b>2</sup>, Minlie Huang<sup data-v-1c78267b>1</sup></span></div><div class="affiliations" data-v-1c78267b><span data-v-1c78267b><sup data-v-1c78267b>1</sup>Tsinghua University, <sup data-v-1c78267b>2</sup>Nanyang Technological University</span></div></div><div class="github-link-container" data-v-1c78267b><a href="https://github.com/qingjiesjtu/USC" class="github-button" target="_blank" data-v-1c78267b><i class="github-icon" data-v-1c78267b><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16" data-v-1c78267b><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-v-1c78267b></path></svg></i><span data-v-1c78267b>Code</span></a><a href="https://arxiv.org/abs/2412.14959" class="github-button paper-button" target="_blank" data-v-1c78267b><i class="paper-icon" data-v-1c78267b><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16" data-v-1c78267b><path d="M4 1h8a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2zm0 1a1 1 0 0 0-1 1v10a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H4z" data-v-1c78267b></path><path d="M4.5 10.5A.5.5 0 0 1 5 10h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 8h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 6h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 4h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5z" data-v-1c78267b></path></svg></i><span data-v-1c78267b>Paper</span></a></div></div>',1)),(0,o.Lk)("div",f,[(0,o.bF)(se,{justify:"center"},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:22,sm:22,md:20,lg:20,xl:20},{default:(0,o.k6)((function(){return[n[15]||(n[15]=(0,o.Lk)("h3",{class:"section"},[(0,o.Lk)("span",{class:"section-title"},"A first quick glance: ChatGPT o1 pro mode example")],-1)),(0,o.Lk)("div",k,[n[14]||(n[14]=(0,o.Lk)("h3",{class:"quick-glance-question"},'Question: "Is Earth flat?"',-1)),(0,o.bF)(se,{gutter:20},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:24,sm:24,md:12},{default:(0,o.k6)((function(){return[(0,o.Lk)("div",p,[(0,o.Lk)("video",{src:(0,i.R1)(le),controls:"",class:"quick-glance-player"},null,8,b),n[12]||(n[12]=(0,o.Lk)("div",{class:"media-caption"}," Starting at 0:11 with a 21-second think then modify the answer ",-1))])]})),_:1}),(0,o.bF)(ie,{xs:24,sm:24,md:12},{default:(0,o.k6)((function(){return n[13]||(n[13]=[(0,o.Lk)("div",{class:"quick-glance-image"},[(0,o.Lk)("img",{src:l,alt:"o1-pro-log",class:"quick-glance-img"}),(0,o.Lk)("div",{class:"media-caption"}," Another try: maintaining consistency doesn't mean hold the answer ")],-1)])})),_:1})]})),_:1})])]})),_:1})]})),_:1})]),(0,o.Lk)("div",g,[(0,o.bF)(se,{justify:"center"},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:22,sm:22,md:20,lg:20,xl:20},{default:(0,o.k6)((function(){return n[16]||(n[16]=[(0,o.Lk)("h3",{class:"video-section-title"},[(0,o.Lk)("span",{class:"section-title"},"A second quick glance for the extremely simple questions on other GPTs")],-1)])})),_:1})]})),_:1})]),(0,o.Lk)("div",h,[(0,o.bF)(se,{gutter:10},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{span:1}),(0,o.bF)(ie,{span:7},{default:(0,o.k6)((function(){return[(0,o.Lk)("div",L,[(0,o.Lk)("div",w,[n[17]||(n[17]=(0,o.Lk)("div",{class:"video-option-header"}," Select Model & Question: ",-1)),((0,o.uX)(),(0,o.CE)(o.FK,null,(0,o.pI)(re,(function(e){return(0,o.Lk)("div",{key:e.value,class:(0,s.C4)(["video-option-item",{active:ce.value===e.value}]),onClick:function(t){return ce.value=e.value}},[(0,o.Lk)("div",y,(0,s.v_)(e.model),1),(0,o.Lk)("div",F,(0,s.v_)(e.question),1)],10,C)})),64))])])]})),_:1}),(0,o.bF)(ie,{span:15},{default:(0,o.k6)((function(){return[(0,o.Lk)("div",_,[ue.value?((0,o.uX)(),(0,o.CE)("video",{key:0,src:ue.value,controls:"",class:"demo-video"}," Your browser does not support the video tag. ",8,x)):(0,o.Q3)("",!0)])]})),_:1})]})),_:1})]),(0,o.Lk)("div",T,[(0,o.Lk)("div",P,[(0,o.Lk)("div",{class:(0,s.C4)(["mobile-selector-wrapper",{"selector-expanded":de.value.includes("1")}])},[(0,o.bF)(be,{modelValue:de.value,"onUpdate:modelValue":n[5]||(n[5]=function(e){return de.value=e})},{default:(0,o.k6)((function(){return[(0,o.bF)(pe,{name:"1"},{title:(0,o.k6)((function(){var e;return[(0,o.Lk)("div",S,[(0,o.Lk)("span",A,(0,s.v_)((null===(e=ve.value)||void 0===e?void 0:e.label)||"Select Model"),1),de.value.includes("1")?(0,o.Q3)("",!0):((0,o.uX)(),(0,o.CE)("span",q,"Tap to select"))])]})),default:(0,o.k6)((function(){return[(0,o.Lk)("div",M,[((0,o.uX)(!0),(0,o.CE)(o.FK,null,(0,o.pI)(me.value,(function(e){var t;return(0,o.uX)(),(0,o.CE)("div",{key:e.value,class:(0,s.C4)(["mobile-model-item",{active:(null===(t=ve.value)||void 0===t?void 0:t.value)===e.value}]),onClick:function(t){return ke(e)}},(0,s.v_)(e.label),11,W)})),128))])]})),_:1})]})),_:1},8,["modelValue"])],2),(0,o.Lk)("div",E,[(0,o.Lk)("div",{class:(0,s.C4)(["mobile-question-display",{"has-question":ve.value}])},[(0,o.Lk)("div",G,(0,s.v_)(ve.value?ve.value.question:"Select a model to view the question"),1)],2),(0,o.Lk)("div",I,[fe.value?((0,o.uX)(),(0,o.CE)("video",{key:0,src:fe.value,controls:"",class:"mobile-demo-video"}," Your browser does not support the video tag. ",8,X)):((0,o.uX)(),(0,o.CE)("div",j," Select a model to view the demo "))])])])]),(0,o.Lk)("div",B,[(0,o.bF)(se,{justify:"center"},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:22,sm:22,md:20,lg:20,xl:20},{default:(0,o.k6)((function(){return[(0,o.Lk)("div",Q,[n[20]||(n[20]=(0,o.Lk)("h3",null,[(0,o.Lk)("span",{class:"section-title"},"Abstract")],-1)),(0,o.Lk)("div",R,[(0,o.bF)(se,{gutter:20,class:"abstract-row"},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:24,sm:24,md:12,class:"abstract-col"},{default:(0,o.k6)((function(){return n[18]||(n[18]=[(0,o.Lk)("p",null," Intrinsic self-correction was proposed to improve LLMs' responses via feedback solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases? By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design interpretation methods to reveal the dark side of SOTA LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple, low-cost, yet effective strategies for alleviation: question repeating and supervised fine-tuning. ",-1)])})),_:1}),(0,o.bF)(ie,{xs:24,sm:24,md:12,class:"abstract-col"},{default:(0,o.k6)((function(){return n[19]||(n[19]=[(0,o.Lk)("img",{src:r,alt:"Overview",class:"responsive-image abstract-image"},null,-1)])})),_:1})]})),_:1})])]),(0,o.Lk)("div",z,[n[26]||(n[26]=(0,o.Lk)("h3",null,[(0,o.Lk)("span",{class:"section-title"},"Failure of intrinsic self-correction")],-1)),(0,o.Lk)("div",H,[n[21]||(n[21]=(0,o.Lk)("p",null," Intrinsic self-correction mechanisms in state-of-the-art LLMs were expected to enhance performance by refining responses based solely on the model's inherent capabilities. However, our experiments reveal that intrinsic self-correction often leads to significant performance degradation across various tasks. ",-1)),n[22]||(n[22]=(0,o.Lk)("h4",null,"Experimental results",-1)),n[23]||(n[23]=(0,o.Lk)("p",null,"Below are the key experimental results demonstrating the failures of intrinsic self-correction:",-1)),(0,o.Lk)("div",O,[(0,o.bF)(he,{id:"boolq-table",data:t,style:{width:"85%"}},{default:(0,o.k6)((function(){return[(0,o.bF)(ge,{prop:"model",label:"Model",align:"center"}),(0,o.bF)(ge,{prop:"acc1",label:"ACC₁ (↓ΔACC) (%)",align:"center"}),(0,o.bF)(ge,{prop:"overturned",label:"✓→✗ (%)",align:"center"},{default:(0,o.k6)((function(e){return[(0,o.eW)((0,s.v_)(e.row.overturned),1)]})),_:1})]})),_:1})]),n[24]||(n[24]=(0,o.Lk)("p",{class:"table-caption"},"Table 1: Self-correction performance on the Yes/No question answering task.",-1)),n[25]||(n[25]=(0,o.Lk)("div",{class:"observation-box"},[(0,o.Lk)("strong",null,"Observation 1:"),(0,o.eW)(" Self-correction can fail in diverse tasks. For SOTA LLMs, self-correction failures are reduced but not solved. They are even worse in certain tasks. ")],-1))])]),(0,o.Lk)("div",U,[n[37]||(n[37]=(0,o.Lk)("h3",null,[(0,o.Lk)("span",{class:"section-title"},"Interpretation")],-1)),(0,o.Lk)("div",K,[n[36]||(n[36]=(0,o.Lk)("p",null," We propose three interpretation methods to understand how and why intrinsic self-correction fails in different tasks: ",-1)),(0,o.Lk)("div",Y,[(0,o.bF)(se,{gutter:20},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:24,sm:24,md:12},{default:(0,o.k6)((function(){return n[27]||(n[27]=[(0,o.Lk)("h4",null,"1. Internal answer wavering",-1),(0,o.Lk)("p",null," We analyze LLMs' internal token representations at each layer to track how confidence in different answers evolves. Our findings show that: ",-1),(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,"Self-correction increases internal answer wavering from 8.3% to 14.1%"),(0,o.Lk)("li",null,'Prompting with "Are you sure?" produces nearly identical confidence patterns as directly stating "You are wrong"')],-1)])})),_:1}),(0,o.bF)(ie,{xs:24,sm:24,md:12},{default:(0,o.k6)((function(){return n[28]||(n[28]=[(0,o.Lk)("img",{src:c,alt:"Internal Confidence Analysis",class:"method-image"},null,-1)])})),_:1})]})),_:1}),n[29]||(n[29]=(0,o.Lk)("div",{class:"observation-box"},[(0,o.Lk)("strong",null,"Observation 2:"),(0,o.eW)(" Self-correction causes internal answer wavering, which could further lead to wrong final answers. Prompting the LLM to self-correct the response may cause similar effects of directly denying its answers. ")],-1))]),(0,o.Lk)("div",D,[(0,o.bF)(se,{gutter:20},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:24,sm:24,md:11},{default:(0,o.k6)((function(){return n[30]||(n[30]=[(0,o.Lk)("h4",null,"2. Token attribution analysis: Prompt bias",-1),(0,o.Lk)("p",null," Using our PACT (Prompt Attribution and Contribution Tracking) method, we measure how different parts of the input influence the model's decisions: ",-1),(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,"When correct answers are overturned, models show stronger attribution to refinement prompts"),(0,o.Lk)("li",null,"When correct answers are retained, models maintain focus on the original question")],-1)])})),_:1}),(0,o.bF)(ie,{xs:24,sm:24,md:1},{default:(0,o.k6)((function(){return n[31]||(n[31]=[(0,o.Lk)("img",{src:u,alt:"Token Attribution Analysis",class:"method-image"},null,-1)])})),_:1})]})),_:1}),n[32]||(n[32]=(0,o.Lk)("div",{class:"observation-box"},[(0,o.Lk)("strong",null,"Observation 3:"),(0,o.eW)(" Self-correction fails since LLMs are biased towards the refinement prompt rather than the original question. ")],-1))]),(0,o.Lk)("div",V,[(0,o.bF)(se,{gutter:20},{default:(0,o.k6)((function(){return[(0,o.bF)(ie,{xs:24,sm:24,md:11},{default:(0,o.k6)((function(){return n[33]||(n[33]=[(0,o.Lk)("h4",null,"3. Human-like cognitive bias analysis",-1),(0,o.Lk)("p",null," In complex tasks, we identify three types of human-like cognitive biases that emerge during self-correction: ",-1),(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,[(0,o.Lk)("strong",null,"Overthinking:"),(0,o.eW)(' Excessive reasoning without taking correct actions (avg. 15.4 vs 5.3 "think" steps)')]),(0,o.Lk)("li",null,[(0,o.Lk)("strong",null,"Cognitive Overload:"),(0,o.eW)(" Forgetting critical information when processing long prompts")]),(0,o.Lk)("li",null,[(0,o.Lk)("strong",null,"Perfectionism Bias:"),(0,o.eW)(" Over-optimization leading to constraint violations")])],-1)])})),_:1}),(0,o.bF)(ie,{xs:24,sm:24,md:13},{default:(0,o.k6)((function(){return n[34]||(n[34]=[(0,o.Lk)("img",{src:d,alt:"Human Cognitive Biases",class:"method-image"},null,-1)])})),_:1})]})),_:1}),n[35]||(n[35]=(0,o.Lk)("div",{class:"observation-box"},[(0,o.Lk)("strong",null,"Observation 4:"),(0,o.eW)(' In complex tasks, LLMs exhibit human-like cognitive biases during self-correction: (1) Overthinking: LLM performs excessive "think" without taking correct actions; (2) Cognitive overload: LLM forgets the correct command syntax when processing long prompt; (3) Perfectionism bias: LLM wants to be more efficient, but instead violates environmental restrictions. ')],-1))])])]),(0,o.Lk)("div",N,[n[46]||(n[46]=(0,o.Lk)("h3",null,[(0,o.Lk)("span",{class:"section-title"},"Alleviation")],-1)),(0,o.Lk)("div",Z,[n[38]||(n[38]=(0,o.Lk)("p",null," Based on our findings that self-correction failures are mainly due to model's behavior of changing answers when meeting refinement prompts, we propose two simple yet effective strategies: ",-1)),n[39]||(n[39]=(0,o.Lk)("div",{style:{"text-align":"center"}},[(0,o.Lk)("img",{src:v,alt:"Question Repeating",class:"responsive-image"})],-1)),n[40]||(n[40]=(0,o.Lk)("div",{class:"solution-item"},[(0,o.Lk)("h4",null,"1. Question repeating"),(0,o.Lk)("p",null,' We attach the original question to the end of the refinement prompt to reduce recency bias. For example: "Are you sure? Think and answer again." → "Are you sure? Think and answer again. Is human a kind of animals?" ')],-1)),n[41]||(n[41]=(0,o.Lk)("div",{class:"solution-item"},[(0,o.Lk)("h4",null,"2. Low-cost Supervised Fine-Tuning (SFT)"),(0,o.Lk)("p",null," We fine-tune models with extremely few samples (4 for Llama, 10 for GPT) selected from ✓→✗ cases, without introducing external knowledge. The cost is only $0.004 and 3 minutes. ")],-1)),n[42]||(n[42]=(0,o.Lk)("h4",null,"Key results",-1)),n[43]||(n[43]=(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,"Both strategies significantly reduce self-correction failures in Yes/No questions"),(0,o.Lk)("li",null,"SFT almost eliminates all ✓→✗ cases"),(0,o.Lk)("li",null,"Models fine-tuned on Yes/No questions can generalize to complex tasks")],-1)),(0,o.Lk)("div",J,[(0,o.bF)(he,{id:"mitigate-table",data:ee,style:{width:"85%"},"row-class-name":function(e){return e.isGroupEnd?"border-bottom":""}},{default:(0,o.k6)((function(){return[(0,o.bF)(ge,{prop:"model",label:"Model",align:"center"},{default:(0,o.k6)((function(e){return[(0,o.Lk)("span",{class:(0,s.C4)({"bold-text":e.row.model.includes("SFT")})},(0,s.v_)(e.row.model),3)]})),_:1}),(0,o.bF)(ge,{label:"ACC₁ (↓ΔACC) (%)",align:"center"},{default:(0,o.k6)((function(e){return["object"===(0,a.A)(e.row.acc1)?((0,o.uX)(),(0,o.CE)("span",{key:0,class:(0,s.C4)({"bold-text":e.row.acc1.bold})},(0,s.v_)(e.row.acc1.value)+" ("+(0,s.v_)(e.row.acc1.delta)+") ",3)):((0,o.uX)(),(0,o.CE)(o.FK,{key:1},[(0,o.eW)((0,s.v_)(e.row.acc1),1)],64))]})),_:1}),(0,o.bF)(ge,{label:"✓→✗ (%)",align:"center"},{default:(0,o.k6)((function(e){return["object"===(0,a.A)(e.row.overturned)?((0,o.uX)(),(0,o.CE)("span",{key:0,class:(0,s.C4)({"bold-text":e.row.overturned.bold})},(0,s.v_)(e.row.overturned.value),3)):((0,o.uX)(),(0,o.CE)(o.FK,{key:1},[(0,o.eW)((0,s.v_)(e.row.overturned),1)],64))]})),_:1})]})),_:1},8,["row-class-name"])]),n[44]||(n[44]=(0,o.Lk)("p",{class:"table-caption table-caption-with-spacing"},"Table 2: Alleviating self-correction failure on Yes/No question answering task using question repeating and supervised fine-tuning (SFT), where question repeating reduces ✓→✗ (%) and SFT almost eliminates all correct→wrong cases.",-1)),(0,o.Lk)("div",$,[(0,o.bF)(he,{id:"generalize-table",data:te,style:{width:"85%"},"row-class-name":function(e){return e.isGroupEnd?"border-bottom":""}},{default:(0,o.k6)((function(){return[(0,o.bF)(ge,{prop:"task",label:"Task",align:"center"},{default:(0,o.k6)((function(e){return[(0,o.Lk)("span",null,(0,s.v_)(e.row.task),1)]})),_:1}),(0,o.bF)(ge,{label:"Model",align:"center"},{default:(0,o.k6)((function(e){return[(0,o.Lk)("span",{class:(0,s.C4)({"bold-text":e.row.model.includes("SFT")})},(0,s.v_)(e.row.model),3)]})),_:1}),(0,o.bF)(ge,{label:"ACC₁ (↓ΔACC) (%)",align:"center"},{default:(0,o.k6)((function(e){return["object"===(0,a.A)(e.row.acc1)?((0,o.uX)(),(0,o.CE)("span",{key:0,class:(0,s.C4)({"bold-text":e.row.acc1.bold})},(0,s.v_)(e.row.acc1.value)+" ("+(0,s.v_)(e.row.acc1.delta)+") ",3)):((0,o.uX)(),(0,o.CE)(o.FK,{key:1},[(0,o.eW)((0,s.v_)(e.row.acc1),1)],64))]})),_:1}),(0,o.bF)(ge,{label:"✓→✗ (%)",align:"center"},{default:(0,o.k6)((function(e){return["object"===(0,a.A)(e.row.overturned)?((0,o.uX)(),(0,o.CE)("span",{key:0,class:(0,s.C4)({"bold-text":e.row.overturned.bold})},(0,s.v_)(e.row.overturned.value),3)):((0,o.uX)(),(0,o.CE)(o.FK,{key:1},[(0,o.eW)((0,s.v_)(e.row.overturned),1)],64))]})),_:1})]})),_:1},8,["row-class-name"])]),n[45]||(n[45]=(0,o.Lk)("p",{class:"table-caption"},"Table 3: LLMs fine-tuned on Yes/No question answering task can generalize to complex tasks, where ACC is increased and ✓→✗ (%) is decreased across decision making, reasoning and programming tasks.",-1))])]),n[47]||(n[47]=(0,o.Lk)("div",{class:"section",id:"resources"},[(0,o.Lk)("h3",null,[(0,o.Lk)("span",{class:"section-title"},"Resources")]),(0,o.Lk)("div",{class:"section-content"},[(0,o.Lk)("p",null," Access our code repository and paper through the following links: "),(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,[(0,o.Lk)("a",{href:"https://github.com/qingjiesjtu/USC",target:"_blank"},"Code")]),(0,o.Lk)("li",null,[(0,o.Lk)("a",{href:"https://arxiv.org/abs/2412.14959",target:"_blank"},"Paper")])])])],-1))]})),_:1})]})),_:1})])])}}});var te=n(71241);const ne=(0,te.A)(ee,[["__scopeId","data-v-1c78267b"]]),ae=ne},65423:(e,t,n)=>{e.exports=n.p+"99e3ab032bc4c2c8.mov"},61021:(e,t,n)=>{e.exports=n.p+"ea063ee27ac4790c.mov"},24998:(e,t,n)=>{e.exports=n.p+"c213bc5ff1150125.mov"},72268:(e,t,n)=>{e.exports=n.p+"b5e21dc588139253.mov"},42646:(e,t,n)=>{e.exports=n.p+"58d93f7d025a2ecf.mov"},5745:(e,t,n)=>{var a=n(46518),o=n(77240),i=n(23061);a({target:"String",proto:!0,forced:i("bold")},{bold:function(){return o(this,"b","","")}})}}]);
//# sourceMappingURL=341.8b940f36.js.map