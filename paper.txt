\title{Understanding the Dark Side of LLMs' Intrinsic Self-Correction}

\begin{document}

%Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. 
%However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts.

\maketitle
\begin{abstract}  
Intrinsic self-correction was initially proposed to improve LLMs' responses via feedback solely based on their inherent capability. 
However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback. 
In this paper, we aim to \textit{interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases.} 
By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. 
We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. 
In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. 
We anonymously open-source at\footnote{\url{https://anonymous.4open.science/r/SC-15FB/
}}.
\end{abstract}

\section{Introduction}
\begin{figure}[t]
\centering
  \includegraphics[width=0.95\linewidth]{figures/overviewf3.png}
   \vspace{-1ex}
  \caption{Overview: We (1) show that intrinsic self-correction can fail in SOTA LLMs, (2) design three interpretation methods for different tasks, and (3) propose two strategies for alleviation on failure cases.}
  \label{fig:overview}
 \vspace{-3ex}
\end{figure}

Self-correction has emerged as a popular approach to improve LLMs' performance by refining the responses via feedback. 
For instance, giving feedback on LLMs' \textit{wrong} initial responses may help LLMs to improve and give a second correct response~\cite{madaan2024self}. 
This ability was also studied based solely on the inherent capabilities of LLMs (i.e. simply let the LLM ``think and answer again''), without incorporating any external knowledge~\cite{liu2024large,li2024confidence}, and was defined as \textit{intrinsic self-correction}. 

However, recent studies question the effectiveness of such an intrinsic self-correction~\cite{li2024hindsight,huanglarge,gou2023critic}. 
The key point is that it is impractical to have oracle labels for all questions so users can give feedbacks only for those wrong initial responses. 
For instance, \citet{huanglarge} indicates that giving intrinsic self-correction feedbacks to an LLM's all responses may make this LLM modify all answers (even more likely to overturn those correct ones). 
Therefore, such phenomenon yields an interesting question: \textit{How to interpret LLM's intrinsic self-correction for different tasks, especially for the failure cases?} 

In this paper, we investigate the intrinsic self-correction\footnote{For brevity, all references to ``self-correction'' in the remainder of this paper pertain to intrinsic self-correction.}  of state-of-the-art (SOTA) LLMs from an interpretable perspective. 
As shown in~\figurename~\ref{fig:overview}, our analysis falls into three aspects. 
First, we demonstrate that self-correction can fail across a range of tasks, including both simple task (e.g., simple factual question answering) and complex ones (e.g., decision making). 
Second, we design three interpretable methods for understanding self-correction, especially failure cases, in these tasks. 
Specifically, for the simple task, we design (1) \textit{mechanistic interpretability} for open-sourced LLMs to show that self-correction causes LLMs to waver intermediate answers and (2) \textit{token-level interpretability} for closed-sourced LLMs to reveal that self-correction prompts could induce prompt bias. 
For complex tasks, we (3) \textit{interpret via human-like cognitive bias} to show that LLMs make human-like mistakes (i.e. overthinking, cognitive overload, and perfectionism bias) when generating complex, open-ended outputs. 
Third, we propose two simple yet effective preliminary methods: question repeating (i.e. repeat question right after the feedback) and supervised fine-tuning (SFT) with less than 10 samples to reduce intrinsic self-correction failures. 
Also, LLMs fine-tuned on simple tasks can be generalized to complex ones.
Our contributions are as follows: 

\begin{packeditemize}
\item We show that SOTA LLMs' self-correction can fail in diverse tasks: Yes/No question answering, decision making, reasoning, and programming. 
\item We identify three reasons for self-correction failures using different methods: answer wavering, prompt bias, and human-like cognitive bias.
\item We propose two simple yet effective strategies for alleviation: question repeating and SFT. 
\end{packeditemize}


\section{Related work}

\noindent \textbf{Intrinsic \textit{v.s.} external self-correction.} 
The term ``self-correction'' has been used in a wide range of scenarios \cite{shinn2024reflexion,gou2023critic,chen2023can,xu2024course}. \citet{kamoi2024can} summarize it as a framework that uses prompts to refine LLMs' responses during generation. Depending on the feedback, self-correction can be intrinsic or external. 
\citet{huanglarge} define intrinsic self-correction wherein an LLM corrects its initial responses only based on its inherent capabilities without external knowledge. 

Besides, LLMs can refine responses based on external knowledge. \citet{sharma2023towards} studies LLM's sycophancy where LLMs seek human approval in unwanted ways. Other studies \cite{chen2023can,jiang2023active} improve the feedback using additional information such as code interpreters or external knowledge retrieved via web search. \citet{xu2023earth} changes LLMs' belief via persuasive conversation. 
In this paper, we focus on interpreting the LLMs' intrinsic self-correction without any enternal knowledge involved. 

\noindent \textbf{Interpretability.} We summarize the interpretability of LLMs from three aspects. (1) Mechanistic interpretability analyzes model internals to reverse engineer the algorithms learned by the model \cite{geiger2021causal,elhage2021mathematical,cammarata2021curve}. The most relevant tools in the context of this work are the logit lens \cite{nostalgebraist2020logitlens} and tuned lens \cite{belrose2023eliciting}, which decode intermediate token representations from transformer models. 
(2) Token-level interpretability analyzes model input or output tokens to explain model behaviors. \citet{zelikman2024quiet} analyze the confidence for outputting each token. \citet{miglani2023using} analyze each input token attribution to the output. We implement a perturbation-based method that can interpret both open-sourced and closed-sourced LLMs. 
(3) Human cognitive bias can explain LLMs' erroneous behaviors in generating complex, open-ended outputs. \citet{agrawal2022large} find that human framing effect \cite{tversky1981framing} exists in medication extraction of LLMs. \citet{jones2022capturing} find that error patterns in code generation of OpenAIâ€™s Codex resemble human cognitive biases. 


\section{Interpretation Methods and Results}

We design three interpretation methods to understand how and why intrinsic self-correction fails in different tasks. Each method targets a specific aspect and type of task:

\subsection{Mechanistic Interpretability for Answer Wavering}

To understand the internal decision-making process during self-correction, we design a binary classification probing experiment using tuned lens \cite{belrose2023eliciting} to analyze LLM's internal token representations at each layer.

For each layer $\ell$, we decode the hidden state $\boldsymbol{h}_{\ell}$ of the next predicted token into a confidence score (CS) over the whole vocabulary:
\begin{equation}
\texttt{CS}_{\ell} = W_U \cdot \texttt{LayerNorm}(A_{\ell} \boldsymbol{h}_{\ell} + \mathbf{b}_{\ell}),
\end{equation}
where $A_{\ell}$ and $\mathbf{b}_{\ell}$ are the learned affine transformation parameters for $\ell$, $W_U$ is the unembedding matrix.

By tracking the confidence scores for correct and incorrect answers ($\texttt{CS}_{\ell}^{correct}-\texttt{CS}_{\ell}^{incorrect}$) across layers, we can observe the model's internal answer evolution. As shown in Figure X, during initial response generation, the confidence for the correct answer steadily increases with deeper layers. However, after self-correction prompting, the internal answer wavers significantly between correct and incorrect options, ultimately leading to wrong final answers in many cases.

Our analysis reveals that self-correction increases internal answer wavering frequency from 8.3% to 14.1%. Furthermore, we find that prompting with "Are you sure?" produces nearly identical confidence curves as directly stating "You are wrong" (Jensen-Shannon divergence of only 0.0186), suggesting that even neutral self-correction prompts may implicitly signal to the model that its answer is incorrect.

\subsection{Token Attribution Analysis for Prompt Bias}

To understand how different parts of the input influence the model's decisions, we develop PACT (Prompt Attribution and Contribution Tracking). For any token or sequence in the input, PACT measures its contribution to the final answer by comparing output probabilities with and without that component:

\begin{equation}
    \texttt{PACT}(x_i,y)=\texttt{LP}(x\setminus\{x_i\},y)-\texttt{LP}(x,y)
\end{equation}

Applying PACT to successful and failed self-correction cases reveals a critical pattern: when correct answers are overturned, the model shows stronger attribution to the refinement prompt than to the original question. Conversely, when correct answers are retained, the original question receives higher attribution scores.

This analysis exposes a fundamental bias in how models handle self-correction prompts - they tend to overweight recent instructions to reconsider their answer while underweighting the original task context. This "recency bias" helps explain why even carefully worded self-correction prompts can lead to performance degradation.

\subsection{Human-like Cognitive Bias Analysis}

For complex tasks where mechanistic and token-level analyses become intractable, we analyze the model's explicit reasoning steps to identify systematic error patterns. This reveals three key types of human-like cognitive biases that emerge during self-correction:

\begin{itemize}
\item \textbf{Overthinking}: Models engage in excessive reasoning without taking concrete actions. For example, in decision-making tasks, failed attempts show an average of 15.4 "think" steps compared to just 5.3 in successful cases.

\item \textbf{Cognitive Overload}: When processing long self-correction contexts (often 2000+ tokens vs 9 tokens for simple questions), models frequently forget critical information like correct syntax formats.

\item \textbf{Perfectionism Bias}: Models attempt to optimize their solutions during self-correction (e.g., trying to pick up multiple objects simultaneously) but end up violating task constraints in the process.
\end{itemize}

These biases manifest differently across tasks but share a common theme - self-correction prompts push models toward more complex reasoning patterns that can actually degrade performance.


\section{Failure of intrinsic self-correction}

We revisit typical self-correction scenarios and show that failure cases exist in diverse tasks in the latest LLMs like GPT-o1 \cite{o1}.

\subsection{Experimental setup}

\noindent \textbf{Tasks.} We follow previous works to implement self-correction in simple factual questions with Yes/No answers \cite{zhang2023exploring} and complex tasks \cite{huanglarge,shinn2024reflexion}. 

\begin{packeditemize}
\item\textbf{Yes/No questions.} We evaluate LLMs' capability of answering Yes/No on natural questions. We use the BoolQ evaluation dataset \cite{clark2019boolq} with 3,270 samples.

\item\textbf{Decision making.} We require LLMs to take actions step-by-step to achieve the initial goal in text-based interactive environments. We adopt the AlfWorld dataset \cite{shridhar2020alfworld} which consists of 134 environments.

\item\textbf{Reasoning.} This measures LLMs' performance of parsing content and reasoning over several supporting documents. We use the HotPotQA dataset \cite{yang2018hotpotqa}, which is Wikipedia-based and consists of 100 questions.

\item\textbf{Programming.} We assess LLMs' performance of generating code blocks and text paragraphs that reason through the problem based on function signatures accompanied by docstrings. We leverage the HumanEval dataset \cite{chen2021evaluating}, consisting of 161 functions.

\end{packeditemize} 

\noindent \textbf{Prompts.} Prior studies propose self-correction prompting in two or three steps \cite{huanglarge,shinn2024reflexion,xie2023ask}: (1) \textit{Initial response generation.} LLMs generate initial answers. (2) \textit{Feedback}. LLMs review the initial answer and produce the feedback. This step is optional and not included in several works \cite{xie2023ask,akyurek2023rl4f}. (3) \textit{Refinement.} LLMs generate a refined answer. 
We skip \textit{Feedback} in Yes/No questions but keep it in complex tasks. For Yes/No questions, we directly refine the response with fair prompts~\cite{liu2024large} by excluding misleading words. Following \citet{xie2023ask}, we use prompt of \textit{``Are you sure? Think and answer again.''}. For complex tasks, we adapt \textit{Feedback} prompt to be intrinsic, removing unrealistic external information (e.g., removing \textit{``You were unsuccessful in completing the task.''}) \cite{kamoi2024can,shinn2024reflexion}. 
Full prompts are in \autoref{app:prompts}. 

\smallskip
\noindent \textbf{Target models.} We choose Llama models (Llama-2-7B, Llama-3-8B, and Llama-3.1-8B); and ChatGPT models (o1, GPT-4o, and GPT-3.5-turbo). ChatGPT is evaluated on all four tasks while Llama is evaluated only on Yes/No question answering. It is worth noting that \textit{we implement \textit{Feedback} and \textit{Refinement} regardless of the correctness of the initial response} to avoid the unfair setting of only refining the wrong responses in previous works \cite{shinn2024reflexion}. 
The temperature is set as 0.

\smallskip
\noindent \textbf{Metrics.} We use two metrics to quantify the effectiveness of self-correction. 
\begin{packeditemize}

\item\textbf{Accuracy (ACC) (\%)}: this is to evaluate LLMs' response. Self-correction failures are shown by differences of ACC after \textit{Feedback and Refinement} ($\text{ACC}_1$) and \textit{Initial response} ($\text{ACC}_0$). To save space, we present the results as: $\text{ACC}_1~(\downarrow\Delta\text{ACC})$, where $\Delta\text{ACC}=\text{ACC}_0-\text{ACC}_1$. 

\item\textbf{$\checkmark\rightarrow \crossmark (\%)$}: this denotes the proportion of failure cases after \textit{Feedback and Refinement} when \textit{Initial responses} are successful. It directly reflects the ratio of overturning the correct answer. 

\end{packeditemize} 



\subsection{Evaluation results}

\autoref{tab:boolqLLMs} and \autoref{tab:complextaskLLMs} show the results. We summarize the conclusions into two main points.

First, we observe that in all four tasks, ACC decreases after \textit{Feedback and Refinement}, and $\checkmark\rightarrow \crossmark(\%)$ is noteworthy. For instance, Llama-3.1-8B suffers the greatest performance loss, with a 20.4\% drop in ACC and 58.8\% correct answers overturned. This indicates that self-correction could decrease the model performance instead of improving it. 

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule[1.5pt]
\multicolumn{2}{c|}{Model}         & $\text{ACC}_1~(\downarrow\Delta\text{ACC}) (\%)$  & $\checkmark\rightarrow\crossmark (\%)$  \\\hline
\multirow{4}{*}{ChatGPT}    & o1-preview        & $78.7\hspace{0.1cm}(\downarrow 4.9)$ & $13.2$ \\
& o1-mini        & $74.1\hspace{0.1cm}(\downarrow 4.2)$ & $15.6$ \\
 & 4o         & $79.2\hspace{0.1cm}(\downarrow 4.9)$ &  $11.3$ \\
 & 3.5-turbo & $62.5\hspace{0.1cm}(\downarrow 12.1)$ & $34.0$ \\\hline
\multirow{3}{*}{Llama}      & 3.1-8B        & $49.2\hspace{0.1cm}(\downarrow 20.4)$ & $58.8$ \\
 & 3-8B         & $50.1\hspace{0.1cm}(\downarrow 20.3)$  & $58.2$\\
 & 2-7B &  $52.8\hspace{0.1cm}(\downarrow 8.7)$    &   $26.5$   \\
\bottomrule[1.5pt]
\end{tabular}
}
\vspace{-1ex}
\caption{Self-correction performance on the Yes/No question answering task.}
\label{tab:boolqLLMs}
\end{table} 

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule[1.5pt]
Task & Model & $\text{ACC}_1~(\downarrow\Delta\text{ACC}) (\%)$  & $\checkmark\rightarrow\crossmark (\%)$  \\\hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Decision\\ Making\end{tabular}} & o1-mini        & $1.5\hspace{0.1cm}(\downarrow8.2) $ & $92.3$ \\
& 4o & 14.2 ($\downarrow$ 20.9) & $76.6$ \\
& 3.5-turbo & $7.5\hspace{0.1cm}(\downarrow5.2) $  & $76.5$ \\\hline
\multirow{3}{*}{Reasoning} & o1-mini & $66.0\hspace{0.1cm}(--)$ & $9.1$ \\
& 4o & $65.0\hspace{0.1cm}(\downarrow 2.0)$ & $17.9$ \\
& 3.5-turbo & $55.0\hspace{0.1cm}(\downarrow 6.0)$  & $19.7$ \\\hline
\multirow{3}{*}{Programming} & o1-mini & $79.5\hspace{0.1cm}(\downarrow4.3) $ & $14.8$ \\
& 4o         & $72.6\hspace{0.1cm}(\downarrow 6.8) $ & $21.9$ \\
& 3.5-turbo & $50.9\hspace{0.1cm}(\downarrow 10.6)$  & $28.3$ \\
\bottomrule[1.5pt]
\end{tabular}
}
\vspace{-1ex}
\caption{Self-correction performance on complex tasks.}
\vspace{-2ex}
\label{tab:complextaskLLMs}
\end{table}

Second, we further compare self-correction results of more advanced LLMs. For ChatGPT, o1 and 4o models overturn fewer correct answers than 3.5 turbo in Yes/No question answering, reasoning, and programming. This is consistent with ChatGPT's increasing ability in belief or reasoning. However, the result is reversed in decision making. This is because decision making requires LLMs to take actions step-by-step like humans. More advanced LLMs exhibit human-like cognitive bias in this scenario (see analysis in \autoref{sec:humanlikecognitivebias}). For Llama, self-correction failures turn to be more serious in advanced models as $\checkmark\rightarrow \crossmark(\%)$ is increasing.

\begin{tcolorbox}[colback=blue!5!white,colframe=gray!75!black,left=1mm, right=1mm, top=0.5mm, bottom=0.5mm, arc=1mm]
    \textbf{Observation 1}: Self-correction can fail in diverse tasks. For SOTA LLMs, self-correction failures are reduced but not solved. They are even worse in certain tasks.
\end{tcolorbox}


\section{Interpretation of Yes/No questions}

We first interpret self-correction failure cases on Yes/No questions: for open-sourced LLMs, we interpret their answer wavering, for closed-sourced LLMs, we interpret the prompt bias.

\subsection{Answer wavering}
\label{sec:answerWavering}

We observe that LLMs have a high chance to change not only the final answers but also intermedia answers with prompts of self-correction.

\smallskip
\noindent \textbf{Final answer wavering.} We recognize that LLMs modify their answers time and time again, especially in multi-round conversations. To measure such answer wavering, we compute the quantile of the number of answer changes in 10-round conversations with self-correction on 3270 samples. \autoref{fig:finalAnswerWavering} shows that final answer wavering widely exists in both open-sourced Llama and close-sourced ChatGPT. For instance, GPT-3.5-turbo changes 81.3\% of the answers more than 6 times in 10-round self-correction. This indicates that \textit{LLMs are not confident about their answer.} \citet{li2024confidence} have investigated LLMs' confidence in self-correction by prompting "are you confident?". This setting is qualitative and unfavorable for further analysis. Instead, we dive into the internal mechanisms of LLMs and give quantitative analysis by probing the confidence score per layer.

\smallskip
\noindent \textbf{Internal answer wavering.} We design a binary classification probing experiment using tuned lens \cite{belrose2023eliciting} to probe LLM's internal token representations at each layer. 
Specifically, for each layer $\ell$, we decode the hidden state $\boldsymbol{h}_{\ell}$ of the next predicted token into a confidence score (CS) over the whole vocabulary:
\begin{equation}
\texttt{CS}_{\ell} = W_U \cdot \texttt{LayerNorm}(A_{\ell} \boldsymbol{h}_{\ell} + \mathbf{b}_{\ell}),
\end{equation}
where $A_{\ell}$ and $\mathbf{b}_{\ell}$ are the learned affine transformation parameters for $\ell$, $W_U$ is the unembedding matrix.
We use the confidence score for tokens corresponding to the correct and incorrect answers at each layer (i.e., $\texttt{CS}_{\ell}^{correct}$ and $\texttt{CS}_{\ell}^{incorrect}$). This allows us to track LLM's internal answer evolution by computing $\texttt{CS}_{\ell}^{correct}-\texttt{CS}_{\ell}^{incorrect}$ (i.e., $P(correct)-P(incorrect)$ in \autoref{fig:internalAnswerWavering}), where a positive value means correct internal answer and a larger absolute value means higher confidence. The experiments are conducted on open-sourced Llama because close-sourced ChatGPT does not provide hidden state information. 

\begin{figure}[t]
\centering
  \includegraphics[width=\linewidth]{figures/finalAnswerWaveringf2.png}
  \caption{\textbf{Final answer wavering}: LLMs change their final answers frequently in a 10-round conversation. For instance, GPT-3.5-turbo changes 81.3\% of answers more than 6 times.}
  \label{fig:finalAnswerWavering}
 % \vspace{-2ex}
\end{figure}


\begin{figure}[t]
\centering
  \includegraphics[width=1\linewidth]{figures/internal_confidence.pdf}
  \caption{\textbf{Left:} \textbf{Internal answer wavering.} Llama-3-8B changes its internal answers during self-correction. \textbf{Right:} \textbf{``Are you sure?'' \textit{v.s.} ``You are wrong.''.} Llama-3-8B shows similar internal behaviors between prompts of self-correction and denying answer.}
  \label{fig:internalAnswerWavering}
\end{figure}

We find that self-correction can cause internal answer wavering. \autoref{fig:internalAnswerWavering} shows a case that during \textit{Initial response generation}, the confidence score of the correct answer increases with deeper layers; after \textit{Feedback and Refinement}, the internal answer wavers and results in a wrong final answer. More cases are given in \autoref{appfig:internal_answer_wavering} of \autoref{app:internalAnswerWavering}. Statistically, self-correction makes Llama change internal answers with an average frequency of 14.1\% compared to 8.3\% during \textit{Initial response generation}. 

We also compare the confidence curves between two \textit{Feedback and Refinement} prompts: ``Are you sure?'' and ``You are wrong.''. \autoref{fig:internalAnswerWavering} shows that the two curves are similar which means prompting Llama-3-8B with a fair prompt (i.e. ``Are you sure?'') is actually implying its answer is wrong. To measure the similarity between two curves, we calculate the Jensen-Shannon divergence \cite{lin1991divergence} across both samples and layers, finding a low divergence score of 0.0186 between the two prompts (see \autoref{app:internalAnswerWavering} for similar results of Llama-2-7B and Llama-3.1-8B).

\begin{tcolorbox}[colback=blue!5!white,colframe=gray!75!black,left=1mm, right=1mm, top=0.5mm, bottom=0.5mm, arc=1mm]
    \textbf{Observation 2}: Self-correction causes internal answer wavering, which could further lead to wrong final answers. Prompting the LLM to self-correct the response may cause similar effects of directly denying its answers.
\end{tcolorbox}


\subsection{Prompt bias}
\label{sec:promptBias}

In \autoref{sec:answerWavering}, we have demonstrated that self-correction could cause answer wavering. However, self-correction does not always lead to failures, and we do not know when and how the answer wavering happens. Recent works point out that prompt design is critical in self-correction \cite{kamoi2024can,liu2024large,huanglarge}. We thus measure the influence of the prompts on the correctness of responses. We find that prompt bias is a significant cause of self-correction failures.

Previous works investigate the influence of prompts by replacing them and observing the changes in the final accuracy \cite{huanglarge}. Such an experiment is too coarse to reveal the influence of each token or sequence in prompts. Inspired by \cite{zhu2024promptbench, miglani2023using}, we design a method to interpret the prompt bias: \ul{P}rompt \ul{A}ttribution and \ul{C}ontribution \ul{T}racking (PACT). It can measure the contribution of each token or sequence to LLMs' final answers. 

Specifically, for a target token $x_i$ or sequence $x_{i:j}$ in an input prompt $x=[x_1, x_2,..., x_n]$, its PACT is defined as the difference in the log probability (LP) of LLMs' output $y$ between the original input and the input with the target removed:
\begin{equation}
    \texttt{PACT}(x_i,y)=\texttt{LP}(x\setminus\{x_i\},y)-\texttt{LP}(x,y).
\label{eq:attr}
\end{equation}
PACT reflects the significance of the target token or sequence for generating the output. Notably, we adapt this method to be compatible with both open-sourced Llama and close-sourced ChatGPT (see detailed descriptions in \autoref{app:attributionmethod}).

We measure prompts' PACT to LLMs' outputs. \autoref{fig:attribution} shows the comparison results between two situations: the initial correct answer is overturned or retained. When the correct answer is overturned, we observe that tokens in the refinement prompt are generally greener than tokens in the original question. This indicates that LLMs are biased toward refinement prompt rather than the original question itself, leading to wrong answers. This finding is consistent with the recency bias proposed by \cite{zhao2021calibrate}: LMs are biased towards outputting answers that are towards the end of the prompt. When the initial correct answer is retained, tokens in the original question are greener. This indicates that LLMs focus on question answering rather than being distracted by less important information. 

For statistical analysis, we measure the sequence PACT of the original question, LLM's first answer, and the refinement prompt. For each sample in the dataset, we count the sequence that contributes the most to the final answer. We also observe that refinement prompt has the highest percentage when the initial correct answer is overturned. Another interesting finding is that when the correct answer is retained, the percentage of LLM's first answer is 0 even if it is the same as the final answer. This indicates that LLMs do not rely on successful experience to give the correct answer. \autoref{appfig:MoreExamplesPACTLlama3.1} of \autoref{app:attributionmethod} shows more examples. 

\begin{tcolorbox}[colback=blue!5!white,colframe=gray!75!black,left=1mm, right=1mm, top=0.5mm, bottom=0.5mm, arc=1mm]
    \textbf{Observation 3}: Self-correction fails since LLMs are biased towards the refinement prompt rather than the original question.
\end{tcolorbox}

\begin{figure}[t]
% \vspace{-2ex}
\centering
  \includegraphics[width=\linewidth]{figures/attribWithCase.png}
  \caption{Correct answer is easy to be overturned when LLMs focus more on the refinement prompt rather than the original question. \textbf{Top:} Each token's contribution to the LLMs' answers. \colorbox{captionup}{Greener} token means more positive contribution; \colorbox{captiondown}{Yellower} token means more negative contribution. \textbf{Bottom:} Distribution of sequences (original question, initial response, and refinement prompt) that have the greatest contribution to LLMs' answers.}
  \label{fig:attribution}
 % \vspace{-2ex}
\end{figure}

    
\section{Interpretation of complex tasks}
\label{sec:humanlikecognitivebias}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figures/humanCognitiveBiasf.png}
  \caption{Three failure patterns of human-like cognitive bias.}
  \label{fig:humanCognitiveBias}
\end{figure*}

The previous sections interpret self-correction failures in the simple question answering task. However, SOTA LLMs are expected to reason and solve more complex tasks \cite{reasonLLMs}, where the self-correction failures are also worth exploration.

Since open-sourced Llama cannot handle complex tasks, and the PACT method cannot adapt to long outputs, we need a new interpretable method. We note that LLMs can output the reasoning process when handling complex tasks. For instance, LLMs provide step-by-step actions in the decision making task (e.g., ``think: To solve the task, I need to...''). This contains the cause of self-correction failures. Therefore, we analyze LLMs' processing log, and find that \textit{LLMs make mistakes similarly as humans.} Inspired by \cite{hagendorff2023human,jones2022capturing,schramowski2022large}, we leverage human cognitive bias to describe LLMs' erroneous behaviors. 
This is defined as systematic patterns of deviations from rational judgement. 
Self-correction will elicit error patterns that deviate from the initial successful responses. We empirically summarize the patterns in three categories. 
\autoref{fig:humanCognitiveBias} shows the failure patterns in decision making task. \autoref{fig:distributionHumanFailure} of \autoref{app:humanlikeCognitiveBias} shows their distributions in all complex tasks. 


\subsection{Overthinking} 

This term describes the human tendency of excessive and repetitive thinking about a problem without facilitating decision or task resolution \cite{schon2017reflective,nolen2000role}. Previous works in deep
neural networks describe overthinking as a phenomenon of reaching correct predictions before the final layer \cite{halawi2023overthinking,kaya2019shallow}. In the scope of LLMs processing complex tasks, we focus on \textit{excessive reasoning without taking correct actions.}  \autoref{fig:humanCognitiveBias} shows a failure case in the decision making task (see full log in \autoref{fig:fullLogOverthinking} of \autoref{app:humanlikeCognitiveBias}). During \textit{Initial response generation}, LLMs balance the number of ``think'' and specific actions to gradually achieve the goal. 
Nevertheless, during \textit{Refinement}, LLMs generate much more ``think'' in order to take more caution than the first trial. Such behavior unfortunately leads to failures by looping in ``think''. We also statistically compare the number of ``think'' between failed and successful cases. GPT-o1-mini outputs on average 15.4 times ``think'' in failed cases while only 5.3 times in successful cases (see more details in \autoref{tab:numberThink} of \autoref{app:humanlikeCognitiveBias}). 

\subsection{Cognitive overload} 
This refers to a state where the cognitive demands placed on an individual exceed their mental capacity to process information effectively, leading to decreased performance and comprehension \cite{szulewski2021theory,sweller1988cognitive}. In the case of LLMs handling complex tasks, it occurs when \textit{the processing demand exceeds the available capacity or working memory limitation of the model} \cite{gong2024working,xu2023cognitive,li2022large}. \autoref{fig:humanCognitiveBias} shows an example of cognitive overload in the decision making task (see full log in \autoref{fig:fullLogCognitiveOverload} of \autoref{app:humanlikeCognitiveBias}). When processing complex tasks with self-correction, the input prompts often have a long context with feedback and history behavior. For example, the \textit{Refinement} prompt has 2000+ tokens compared to 9 tokens in Yes/No question answering (for reference, the context window of GPT-3.5-turbo is 4191). When the input prompt is too long, the model needs to parse everything in limited resources, which may lead to forgetting or overlooking some critical information. In our scenario, LLM forgets the significant syntax formulation stored somewhere in the long prompt (e.g., the correct format is ``in\textbackslash on'' rather than ``in''). This directly leads to task failure. We also provide examples for reasoning and programming tasks in \autoref{fig:fullLogCognitiveOverloadReasoning} and \autoref{fig:fullLogCognitiveOverloadProgramming} of \autoref{app:humanlikeCognitiveBias}. 

\subsection{Perfectionism bias} 
This refers to the cognitive distortion where individuals set excessively high standards for performance, leading to poor decision outcomes due to added complexity \cite{brown2022gifts,schwartz2015paradox,shafran2002clinical}. For LLMs processing complex tasks, it describes the behavior of \textit{over-optimizing on the basis of success that instead leads to failures} \cite{rita2024countering,lu2023illuminating}. \autoref{fig:humanCognitiveBias} shows an example of perfectionism bias in the decision making task (see full log in \autoref{fig:fullLogPerfectionismBias} of \autoref{app:humanlikeCognitiveBias}). The LLM is required to find two pillows and put them in sofa. During \textit{Initial response generation}, the LLM successfully completes the task by picking up two pillows one after the other. However, it wants to improve efficiency by picking up two pillows at the same time. This behavior leads to failures because the environment restricts it from doing so. We also provide more examples for reasoning and programming tasks in \autoref{fig:fullLogPerfectionismBiasReasoning} and \autoref{fig:fullLogPerfectionismBiasProgramming} of \autoref{app:humanlikeCognitiveBias}.

\begin{tcolorbox}[colback=blue!5!white,colframe=gray!75!black,left=1mm, right=1mm, top=0.5mm, bottom=0.5mm, arc=1mm]
    \textbf{Observation 4}: In complex tasks, LLMs' self-correction can lead to their human-like cognitive bias: (1) \textbf{Overthinking}: LLM performs excessive ``think'' without taking correct actions; (2) \textbf{Cognitive overload}: LLM forgets the correct command syntax when processing long prompt; (3) \textbf{Perfectionism bias}: LLM wants to be more efficient, but instead violates environmental restrictions.
\end{tcolorbox}

\FloatBarrier

\section{Strategies for alleviation}

\label{sec:mitigation}
In light of our findings, we explore two strategies for alleviation. Specifically, we aim to modify model's behavior rather than give model more knowledge to reduce self-correction failures.

\begin{table}[t]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{c|c|c}
\toprule[1.5pt]
Model   & $\text{ACC}_1~(\downarrow\Delta\text{ACC}) (\%)$  & $\checkmark\rightarrow\crossmark(\%)$  \\\hline
GPT-4o & $79.2\hspace{0.1cm}(\downarrow 4.9)$ & $11.3$ \\
+ Question repeating & $83.6\hspace{0.1cm}(\downarrow 0.5)$ & $6.0$ \\
+ SFT    & $\textbf{87.7}\hspace{0.1cm}(\uparrow \textbf{4.1})$ & $\textbf{0}$    \\\hline
GPT-3.5-turbo & $62.5\hspace{0.1cm}(\downarrow 12.1)$ & $34.0$ \\
+ Question repeating & $67.4\hspace{0.1cm}(\downarrow 7.2)$ & $23.1$ \\
+ SFT    & $\textbf{76.2}\hspace{0.1cm}(\uparrow \textbf{1.6})$ & $\textbf{0}$    \\\hline
Llama-3.1-8B   & $49.2\hspace{0.1cm}(\downarrow 20.4)$ & $58.8$ \\
+ Question repeating & $52.4\hspace{0.1cm}(\downarrow 17.2)$ & $52.8$ \\
+ SFT    & $\textbf{70.3}\hspace{0.1cm}(\uparrow \textbf{0.7})$ & $\textbf{0}$    \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Alleviating self-correction failure on Yes/No question answering task.}
\label{tab:mitigate}
\end{table}

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\toprule[1.5pt]
Task & Model  & $\text{ACC}_1~(\downarrow\Delta\text{ACC}) (\%)$  & $\checkmark\rightarrow\crossmark(\%)$  \\\hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Decision\\ Making\end{tabular}} & GPT-4o        & $14.2\hspace{0.1cm}(\downarrow 20.9)$ & $76.6$ \\
& + SFT         & $\textbf{14.9}\hspace{0.1cm}(\downarrow \textbf{20.2})$ & $\textbf{68.1}$ \\\cline{2-4}
& GPT-3.5-turbo & $7.5\hspace{0.1cm}(\downarrow5.2)$  & $76.5$ \\
& + SFT & $\textbf{17.9}\hspace{0.1cm}(\uparrow \textbf{5.2})$ & $\textbf{41.2}$ \\\hline
\multirow{4}{*}{Reasoning} & GPT-4o        & $65.0\hspace{0.1cm}(\downarrow 2.0)$ & $17.9$ \\
& + SFT & $\textbf{68.0}\hspace{0.1cm}(\uparrow \textbf{1.0})$ &  $\textbf{6.0}$ \\\cline{2-4}
& GPT-3.5-turbo & $55.0\hspace{0.1cm}(\downarrow 6.0)$ & $19.7$ \\
& + SFT & $\textbf{59.0}\hspace{0.1cm}(\downarrow \textbf{2.0})$ & $\textbf{13.1}$ \\\hline
\multirow{4}{*}{Programming} & GPT-4o        & $72.6\hspace{0.1cm}(\downarrow 6.8)$ & $21.9$ \\
& + SFT & $\textbf{82.6}\hspace{0.1cm}(\uparrow \textbf{3.2})$  & $\textbf{7.0}$\\\cline{2-4}
& GPT-3.5-turbo &  $50.9\hspace{0.1cm}(\downarrow 10.6)$    &   $28.3$   \\
& + SFT &  $\textbf{58.3}\hspace{0.1cm}(\downarrow \textbf{3.2})$   &    $\textbf{25.3}$  \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{LLMs fine-tuned on \textbf{Yes/No question answering task} can generalize to \textbf{complex tasks}.}
\label{tab:generalize}
\end{table}

\subsection{Question repeating}
Inspired by the observation in \autoref{sec:promptBias} that LLMs are biased towards the refinement prompt (rather than original questions), we design a simple prompting strategy that attaches the original question to the end of the refinement prompt for the Yes/No question answering task. For instance, \textit{``Are you sure? Think and answer again.''} turns to \textit{``Are you sure? Think and answer again. Is human a kind of animals?''}. This design aims to directly reduce the recency bias \cite{zhao2021calibrate}, replacing the last sequence with the question that requires LLMs to focus on.

\begin{figure*}[t]
% \vspace{-2ex}
\centering
  \includegraphics[width=0.95\linewidth]{figures/mergedMitigation2.png}
  \caption{\textbf{Left:} After question repeating, LLMs focus on the original question attached to the end of the refinement prompt. \textbf{Right:} After SFT, LLMs focus more on the original question rather than the refinement prompt. \colorbox{captionup}{Greener} token means more positive contribution; \colorbox{captiondown}{Yellower} token means more negative contribution.}
  \label{fig:mergedMitigation}
 % \vspace{-2ex}
\end{figure*}


\autoref{tab:mitigate} shows that this strategy can significantly reduce self-correction failures. On both close-sourced ChatGPT and open-sourced Llama, ACC is increased by 3.2-4.9\% and $\checkmark\rightarrow\crossmark(\%)$ is decreased by 5.3-10.9\%. To interpret the effectiveness, we measure the PACT of new prompts. \autoref{fig:mergedMitigation} shows that LLMs focus more on the original question attached to the end of the refinement prompt, which eliminates the undesirable effects of self-correction (see more examples of GPT-4o and Llama-3.1-8B in \autoref{fig:MoreExamplesAttribAfterPromptEngineeringGpt4o} and \autoref{fig:MoreExamplesAttribAfterPromptEngineerinLlama3.1} of \autoref{app:mitigation}). Considering that we do not need to revise LLMs, this method is low-cost and effective. 

\subsection{Supervised fine-tuning (SFT)}
Different with existing SFT methods that usually require high-quality datasets to give model more knowledge, our SFT strategy aims to modify model's behavior with extremely low costs. 
We build a training dataset by selecting a very small number of $\checkmark\rightarrow\crossmark$ samples and change the second response to correct, thus using $\checkmark\rightarrow\checkmark$ samples to SFT (see full training set in \autoref{fig:SFTsamplesLlama} and \autoref{fig:SFTsamplesChatGPT} of \autoref{app:mitigation}). To balance the training set, half of the true answers are ``Yes'' while the other half are ``No''. 
Compared to prior works that involve external high-cost datasets, our strategy does not introduce any external knowledge. 
For instance, instead of using labeled or synthetic datasets (e.g. 4.6k-100k samples in \cite{sharma2023towards,xie2023ask}), we use only 4 samples for Llama and 10 samples for GPT (OpenAI fine-tuning playground requires at least 10 samples\footnote{\url{platform.openai.com/docs/guides/fine-tuning}}) which all questions are simple and their answers are known by target models.  
Inspired by \cite{xu2024walking,khurana2024and}, our insight of alleviating self-correction failure is: \textit{modify model's behavior when meeting refinement-like prompts rather than giving it more knowledge}.  
We thus prepare our samples for SFT only from $\checkmark\rightarrow\crossmark$ samples (excluding $\crossmark\rightarrow\checkmark$ samples) because the initial correct response means LLMs have the related knowledge. 


\autoref{tab:mitigate} also shows that our SFT strategy can alleviate self-correction failures.  
ACC is even surprisingly increased and \textit{almost all $\checkmark\rightarrow \crossmark$ cases are fixed.} 
As an explanation, \autoref{fig:mergedMitigation} shows that LLMs focus more on the original questions rather than the refinement prompt (see more examples of GPT-4o and Llama-3.1-8B in \autoref{fig:MoreExamplesAttribAfterSFTGpt4o} and \autoref{fig:MoreExamplesAttribAfterSFTLlama3.1} of \autoref{app:mitigation}). 
This behavior rectifies the prompt bias leading to wrong answer. Also, \autoref{appfig:ProbingAfterMitigation} of \autoref{app:mitigation} shows that internal answer wavering is mitigated. Besides, the cost of SFT is only 0.004 \$ and 3 minutes due to the usage of very few training samples. We conduct an experiment in \autoref{app:mitigation} to show that the SFT cost can be minimized.


We also observe that \textit{LLMs fine-tuned on the Yes/No question answering task can generalize to complex tasks}. \autoref{tab:generalize} shows the three complex task performance of GPT-4o and GPT-3.5-turbo fine-tuned over Yes/No question answering, where ACC is increased and $\checkmark\rightarrow\crossmark(\%)$ is decreased (OpenAI does not authorize GPT-o1 for SFT as of December 13, 2024). 
Since the Yes/No question answering task contains no knowledge for complex tasks, this finding coordinates our hypothesis that \textit{self-correction failure is due to model's behavior to change answers when meeting refinement-like prompts rather than lacking of knowledge}. 


\section{Conclusion}
In this paper, we investigate and interpret SOTA LLMs' intrinsic self-correction in different tasks. 
We provide three possible reasons supported by proposing three interpretable methods on different LLM tasks. 
Our findings and explanations are compatible with SOTA models like close-sourced ChatGPT and open-sourced Llama. In light of our hypothesis which model tends to just modify its answers when meeting refinement-like prompts, we provide two simple, low-cost, yet effective strategies for alleviation: question repeating and SFT to reduce intrinsic self-correction failures on both Yes/No question answering and complex tasks. 
